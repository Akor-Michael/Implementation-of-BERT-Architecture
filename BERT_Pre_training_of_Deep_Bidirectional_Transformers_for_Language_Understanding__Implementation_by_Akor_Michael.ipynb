{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries and packages\n",
        "import os\n",
        "import requests\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# Install required Python packages using pip\n",
        "!pip install transformers datasets tokenizers\n",
        "\n",
        "# Define the URL for downloading the Cornell Movie Dialogs Corpus\n",
        "data_url = \"http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\"\n",
        "\n",
        "# Define a function to download and extract the dataset\n",
        "def download_and_extract_data(url, target_dir):\n",
        "    # Create the target directory if it doesn't exist\n",
        "    if not os.path.exists(target_dir):\n",
        "        os.makedirs(target_dir)\n",
        "\n",
        "    # Define the file path for saving the downloaded ZIP file\n",
        "    zip_file_path = os.path.join(target_dir, \"cornell_movie_dialogs_corpus.zip\")\n",
        "\n",
        "    # Download the ZIP file from the specified URL\n",
        "    response = requests.get(url)\n",
        "    with open(zip_file_path, \"wb\") as zip_file:\n",
        "        zip_file.write(response.content)\n",
        "\n",
        "    # Extract the downloaded ZIP file quietly\n",
        "    with ZipFile(zip_file_path, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(target_dir)\n",
        "\n",
        "    # Remove the downloaded ZIP file\n",
        "    os.remove(zip_file_path)\n",
        "\n",
        "# Specify the target directory for storing the dataset\n",
        "target_directory = \"./datasets\"\n",
        "\n",
        "# Download and extract the Cornell Movie Dialogs Corpus\n",
        "download_and_extract_data(data_url, target_directory)\n",
        "\n",
        "# Move the necessary dataset files to the datasets directory\n",
        "os.rename(\n",
        "    os.path.join(target_directory, \"cornell movie-dialogs corpus/movie_conversations.txt\"),\n",
        "    os.path.join(target_directory, \"movie_conversations.txt\")\n",
        ")\n",
        "os.rename(\n",
        "    os.path.join(target_directory, \"cornell movie-dialogs corpus/movie_lines.txt\"),\n",
        "    os.path.join(target_directory, \"movie_lines.txt\")\n",
        ")\n"
      ],
      "metadata": {
        "id": "tHktDmcxoQBC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7102177-b8d1-46cc-b95b-b91f5d65e177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.32.1-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.14.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: tokenizers, safetensors, xxhash, dill, multiprocess, huggingface-hub, transformers, datasets\n",
            "Successfully installed datasets-2.14.4 dill-0.3.7 huggingface-hub-0.16.4 multiprocess-0.70.15 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.32.1 xxhash-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Necessary Libraries\n",
        "import os\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import re\n",
        "import random\n",
        "import transformers, datasets\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "from transformers import BertTokenizer\n",
        "import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import itertools\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.optim import Adam"
      ],
      "metadata": {
        "id": "eaGJKuxxJdR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the maximum length for text sequences\n",
        "MAX_LEN = 64\n",
        "\n",
        "# Define file paths for the movie conversation and lines data\n",
        "corpus_movie_conv = './datasets/movie_conversations.txt'\n",
        "corpus_movie_lines = './datasets/movie_lines.txt'\n",
        "\n",
        "# Read the conversation and lines data into memory\n",
        "with open(corpus_movie_conv, 'r', encoding='iso-8859-1') as conv_file:\n",
        "    conversation_data = conv_file.readlines()\n",
        "with open(corpus_movie_lines, 'r', encoding='iso-8859-1') as lines_file:\n",
        "    lines_data = lines_file.readlines()\n",
        "\n",
        "# Create a dictionary to store lines with their respective IDs\n",
        "lines_dict = {}\n",
        "for line in lines_data:\n",
        "    line_parts = line.split(\" +++$+++ \")\n",
        "    lines_dict[line_parts[0]] = line_parts[-1].strip()\n",
        "\n",
        "# Initialize a list to store question-answer pairs\n",
        "pairs = []\n",
        "\n",
        "# Iterate through conversations to create question-answer pairs\n",
        "for conversation in conversation_data:\n",
        "    conversation_ids = eval(conversation.split(\" +++$+++ \")[-1])\n",
        "\n",
        "    # Iterate through the conversation IDs\n",
        "    for i in range(len(conversation_ids)):\n",
        "        qa_pair = []\n",
        "\n",
        "        # Skip the last ID if reached\n",
        "        if i == len(conversation_ids) - 1:\n",
        "            break\n",
        "\n",
        "        # Get the text for the first and second parts of the pair\n",
        "        first_line = lines_dict[conversation_ids[i]].strip()\n",
        "        second_line = lines_dict[conversation_ids[i + 1]].strip()\n",
        "\n",
        "        # Truncate and store the text within the defined maximum length\n",
        "        qa_pair.append(' '.join(first_line.split()[:MAX_LEN]))\n",
        "        qa_pair.append(' '.join(second_line.split()[:MAX_LEN]))\n",
        "\n",
        "        # Add the question-answer pair to the list\n",
        "        pairs.append(qa_pair)\n",
        "\n",
        "# Example: Print a sample question-answer pair\n",
        "print(pairs[30])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ER4dwYJDoZFU",
        "outputId": "9b0e2186-e88d-45ef-b15b-7e10ec8fd0f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Then that's all you had to say.\", 'But']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a directory to store the data\n",
        "os.mkdir('./data')\n",
        "\n",
        "# Initialize lists to store text data and a count for the saved files\n",
        "text_data = []\n",
        "file_count = 0\n",
        "\n",
        "# Iterate through the text samples and save them to files\n",
        "for sample in tqdm.tqdm([x[0] for x in pairs]):\n",
        "    text_data.append(sample)\n",
        "\n",
        "    # Check if we have accumulated 10,000 samples, then save to a file\n",
        "    if len(text_data) == 10000:\n",
        "        with open(f'./data/text_{file_count}.txt', 'w', encoding='utf-8') as file:\n",
        "            file.write('\\n'.join(text_data))\n",
        "        text_data = []  # Reset the text_data list\n",
        "        file_count += 1\n",
        "\n",
        "# Get a list of file paths for the saved text files\n",
        "file_paths = [str(file_path) for file_path in Path('./data').glob('**/*.txt')]\n",
        "\n",
        "# Print the total number of saved files\n",
        "print(len(file_paths))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4K2nIFalnOz",
        "outputId": "4c27b717-7539-475f-f9b9-50676637dd5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 221616/221616 [00:00<00:00, 1535818.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a BertWordPieceTokenizer with specified settings\n",
        "tokenizer = BertWordPieceTokenizer(\n",
        "    clean_text=True,              # Clean text\n",
        "    handle_chinese_chars=False,   # Do not handle Chinese characters specially\n",
        "    strip_accents=False,         # Do not strip accents\n",
        "    lowercase=True               # Convert text to lowercase\n",
        ")\n",
        "\n",
        "# Train the tokenizer using the specified parameters\n",
        "tokenizer.train(\n",
        "    files=file_paths,                 # List of files containing text data\n",
        "    vocab_size=30_000,           # Vocabulary size\n",
        "    min_frequency=5,             # Minimum frequency for a word to be included in the vocabulary\n",
        "    limit_alphabet=1000,         # Limit the alphabet size\n",
        "    wordpieces_prefix='##',      # Prefix for wordpieces\n",
        "    special_tokens=['[PAD]', '[CLS]', '[SEP]', '[MASK]', '[UNK]']  # Special tokens\n",
        ")\n",
        "\n",
        "# Create a directory to save the trained tokenizer\n",
        "os.mkdir('./bert-it-1')\n",
        "\n",
        "# Save the trained tokenizer to the specified directory\n",
        "tokenizer.save_model('./bert-it-1', 'bert-it')\n",
        "\n",
        "# Load the tokenizer from the saved model\n",
        "tokenizer = BertTokenizer.from_pretrained('./bert-it-1/bert-it-vocab.txt', local_files_only=True)\n",
        "\n",
        "# Tokenize a sample text and print token IDs and tokens\n",
        "token_ids = tokenizer('I like surfboarding!')['input_ids']\n",
        "print(token_ids)\n",
        "print(tokenizer.convert_ids_to_tokens(token_ids))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzMa6PMWrJZj",
        "outputId": "3c0c0ab8-3a5d-473b-a6c2-461f9637ef57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 48, 250, 4033, 3588, 154, 5, 2]\n",
            "['[CLS]', 'i', 'like', 'surf', '##board', '##ing', '!', '[SEP]']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1756: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, data_pair, tokenizer, seq_len=64):\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.seq_len = seq_len\n",
        "        self.corpus_lines = len(data_pair)\n",
        "        self.lines = data_pair\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.corpus_lines\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "\n",
        "        # Step 1: Get a random sentence pair, either negative or positive (saved as is_next_label)\n",
        "        t1, t2, is_next_label = self.get_sent(item)\n",
        "\n",
        "        # Step 2: Replace random words in sentences with [MASK] or random words\n",
        "        t1_random, t1_label = self.random_word(t1)\n",
        "        t2_random, t2_label = self.random_word(t2)\n",
        "\n",
        "        # Step 3: Add [CLS] and [SEP] tokens to the start and end of sentences\n",
        "        # Add [PAD] tokens for labels\n",
        "        t1 = [self.tokenizer.vocab['[CLS]']] + t1_random + [self.tokenizer.vocab['[SEP]']]\n",
        "        t2 = t2_random + [self.tokenizer.vocab['[SEP]']]\n",
        "        t1_label = [self.tokenizer.vocab['[PAD]']] + t1_label + [self.tokenizer.vocab['[PAD]']]\n",
        "        t2_label = t2_label + [self.tokenizer.vocab['[PAD]']]\n",
        "\n",
        "        # Step 4: Combine sentence 1 and 2 as one input\n",
        "        # Add [PAD] tokens to make the sentence the same length as seq_len\n",
        "        segment_label = ([1 for _ in range(len(t1))] + [2 for _ in range(len(t2))])[:self.seq_len]\n",
        "        bert_input = (t1 + t2)[:self.seq_len]\n",
        "        bert_label = (t1_label + t2_label)[:self.seq_len]\n",
        "        padding = [self.tokenizer.vocab['[PAD]'] for _ in range(self.seq_len - len(bert_input))]\n",
        "        bert_input.extend(padding), bert_label.extend(padding), segment_label.extend(padding)\n",
        "\n",
        "        output = {\"bert_input\": bert_input,\n",
        "                  \"bert_label\": bert_label,\n",
        "                  \"segment_label\": segment_label,\n",
        "                  \"is_next\": is_next_label}\n",
        "\n",
        "        return {key: torch.tensor(value) for key, value in output.items()}\n",
        "\n",
        "    def random_word(self, sentence):\n",
        "        tokens = sentence.split()\n",
        "        output_label = []\n",
        "        output = []\n",
        "\n",
        "        # 15% of the tokens would be replaced\n",
        "        for i, token in enumerate(tokens):\n",
        "            prob = random.random()\n",
        "\n",
        "            # Remove [CLS] and [SEP] tokens\n",
        "            token_id = self.tokenizer(token)['input_ids'][1:-1]\n",
        "\n",
        "            if prob < 0.15:\n",
        "                prob /= 0.15\n",
        "\n",
        "                # 80% chance to change token to [MASK]\n",
        "                if prob < 0.8:\n",
        "                    for i in range(len(token_id)):\n",
        "                        output.append(self.tokenizer.vocab['[MASK]'])\n",
        "\n",
        "                # 10% chance to change token to random token\n",
        "                elif prob < 0.9:\n",
        "                    for i in range(len(token_id)):\n",
        "                        output.append(random.randrange(len(self.tokenizer.vocab)))\n",
        "\n",
        "                # 10% chance to keep the token unchanged\n",
        "                else:\n",
        "                    output.append(token_id)\n",
        "\n",
        "                output_label.append(token_id)\n",
        "\n",
        "            else:\n",
        "                output.append(token_id)\n",
        "                for i in range(len(token_id)):\n",
        "                    output_label.append(0)\n",
        "\n",
        "        # Flattening\n",
        "        output = list(itertools.chain(*[[x] if not isinstance(x, list) else x for x in output]))\n",
        "        output_label = list(itertools.chain(*[[x] if not isinstance(x, list) else x for x in output_label]))\n",
        "        assert len(output) == len(output_label)\n",
        "        return output, output_label\n",
        "\n",
        "    def get_sent(self, index):\n",
        "        '''Return a random sentence pair, either negative or positive (for next sentence prediction)'''\n",
        "        t1, t2 = self.get_corpus_line(index)\n",
        "\n",
        "        # Negative or positive pair, for next sentence prediction\n",
        "        if random.random() > 0.5:\n",
        "            return t1, t2, 1\n",
        "        else:\n",
        "            return t1, self.get_random_line(), 0\n",
        "\n",
        "    def get_corpus_line(self, item):\n",
        "        '''Return a sentence pair'''\n",
        "        return self.lines[item][0], self.lines[item][1]\n",
        "\n",
        "    def get_random_line(self):\n",
        "        '''Return a random single sentence'''\n",
        "        return self.lines[random.randrange(len(self.lines))][1]\n"
      ],
      "metadata": {
        "id": "bXCsfR3tmajw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a BERTDataset instance for training data\n",
        "train_data = BERTDataset(pairs, seq_len=MAX_LEN, tokenizer=tokenizer)\n",
        "\n",
        "# Create a DataLoader for training data\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, pin_memory=True)\n",
        "\n",
        "# Get a sample batch from the training data loader\n",
        "sample_data = next(iter(train_loader))\n",
        "print('Batch Size:', sample_data['bert_input'].size())\n",
        "\n",
        "# Get a random example from the training data\n",
        "result = train_data[random.randrange(len(train_data))]\n",
        "\n",
        "# Print the result\n",
        "result\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RD5ta6KA_wK",
        "outputId": "1aee0fa4-9400-495d-fd0f-60143f6bf65b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Size: torch.Size([32, 64])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bert_input': tensor([    1, 19137,  1056,     3,   408,  1083,    17,     2,   335,    16,\n",
              "           179,   182,    11,    58,   243, 11307,     3,     3,     3,     2,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0]),\n",
              " 'bert_label': tensor([  0,  48,   0, 266,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0, 253, 162,  34,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0]),\n",
              " 'segment_label': tensor([1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'is_next': tensor(0)}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(torch.nn.Module):\n",
        "    def __init__(self, d_model, max_len=128):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initialize positional encodings matrix\n",
        "        pe = torch.zeros(max_len, d_model).float()\n",
        "        pe.requires_grad = False\n",
        "\n",
        "        for pos in range(max_len):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i) / d_model)))\n",
        "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1)) / d_model)))\n",
        "\n",
        "        # Include the batch size dimension\n",
        "        self.pe = pe.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pe\n",
        "\n",
        "class BERTEmbedding(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, seq_len=64, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embed_size = embed_size\n",
        "\n",
        "        # Token embedding layer\n",
        "        self.token = torch.nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
        "\n",
        "        # Segment embedding layer\n",
        "        self.segment = torch.nn.Embedding(3, embed_size, padding_idx=0)\n",
        "\n",
        "        # Positional embedding layer\n",
        "        self.position = PositionalEmbedding(d_model=embed_size, max_len=seq_len)\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = torch.nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, sequence, segment_label):\n",
        "        # Combine token, positional, and segment embeddings\n",
        "        x = self.token(sequence) + self.position(sequence) + self.segment(segment_label)\n",
        "        return self.dropout(x)\n",
        "\n",
        "# Testing the embedding layer\n",
        "embed_layer = BERTEmbedding(vocab_size=len(tokenizer.vocab), embed_size=768, seq_len=MAX_LEN)\n",
        "embed_result = embed_layer(sample_data['bert_input'], sample_data['segment_label'])\n",
        "print(embed_result.size())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6Nr8gMt49dF",
        "outputId": "c16211cb-6cab-4c8d-a5f5-5999fd9929b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 64, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadedAttention(torch.nn.Module):\n",
        "    def __init__(self, heads, d_model, dropout=0.1):\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "\n",
        "        assert d_model % heads == 0\n",
        "        self.d_k = d_model // heads\n",
        "        self.heads = heads\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "        self.query = torch.nn.Linear(d_model, d_model)\n",
        "        self.key = torch.nn.Linear(d_model, d_model)\n",
        "        self.value = torch.nn.Linear(d_model, d_model)\n",
        "        self.output_linear = torch.nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, query, key, value, mask):\n",
        "        query = self.query(query)\n",
        "        key = self.key(key)\n",
        "        value = self.value(value)\n",
        "\n",
        "        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)\n",
        "        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)\n",
        "        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)\n",
        "\n",
        "        scores = torch.matmul(query, key.permute(0, 1, 3, 2)) / math.sqrt(query.size(-1))\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        weights = self.dropout(weights)\n",
        "\n",
        "        context = torch.matmul(weights, value)\n",
        "\n",
        "        context = context.permute(0, 2, 1, 3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n",
        "\n",
        "        return self.output_linear(context)\n",
        "\n",
        "class FeedForward(torch.nn.Module):\n",
        "    def __init__(self, d_model, middle_dim=2048, dropout=0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "\n",
        "        self.fc1 = torch.nn.Linear(d_model, middle_dim)\n",
        "        self.fc2 = torch.nn.Linear(middle_dim, d_model)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.activation = torch.nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.activation(self.fc1(x))\n",
        "        out = self.fc2(self.dropout(out))\n",
        "        return out\n",
        "\n",
        "class EncoderLayer(torch.nn.Module):\n",
        "    def __init__(self, d_model=768, heads=12, feed_forward_hidden=768 * 4, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.layernorm = torch.nn.LayerNorm(d_model)\n",
        "        self.self_multihead = MultiHeadedAttention(heads, d_model)\n",
        "        self.feed_forward = FeedForward(d_model, middle_dim=feed_forward_hidden)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, embeddings, mask):\n",
        "        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
        "        interacted = self.layernorm(interacted + embeddings)\n",
        "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
        "        encoded = self.layernorm(feed_forward_out + interacted)\n",
        "        return encoded\n",
        "\n",
        "# Testing the attention layers\n",
        "mask = (sample_data['bert_input'] > 0).unsqueeze(1).repeat(1, sample_data['bert_input'].size(1), 1).unsqueeze(1)\n",
        "transformer_block = EncoderLayer()\n",
        "transformer_result = transformer_block(embed_result, mask)\n",
        "print(transformer_result.size())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7n6FOkOBWK2",
        "outputId": "c069cbd9-5ff5-464a-800c-af1556c0cd1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 64, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BERT(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=768, n_layers=12, heads=12, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_layers = n_layers\n",
        "        self.heads = heads\n",
        "        self.feed_forward_hidden = d_model * 4\n",
        "        self.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=d_model)\n",
        "        self.encoder_blocks = torch.nn.ModuleList([EncoderLayer(d_model, heads, d_model * 4, dropout) for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, x, segment_info):\n",
        "        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
        "        x = self.embedding(x, segment_info)\n",
        "        for encoder in self.encoder_blocks:\n",
        "            x = encoder.forward(x, mask)\n",
        "        return x\n",
        "\n",
        "class NextSentencePrediction(torch.nn.Module):\n",
        "    def __init__(self, hidden):\n",
        "        super().__init__()\n",
        "        self.linear = torch.nn.Linear(hidden, 2)\n",
        "        self.softmax = torch.nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.softmax(self.linear(x[:, 0]))\n",
        "\n",
        "class MaskedLanguageModel(torch.nn.Module):\n",
        "    def __init__(self, hidden, vocab_size):\n",
        "        super().__init__()\n",
        "        self.linear = torch.nn.Linear(hidden, vocab_size)\n",
        "        self.softmax = torch.nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.softmax(self.linear(x))\n",
        "\n",
        "class BERTLM(torch.nn.Module):\n",
        "    def __init__(self, bert: BERT, vocab_size):\n",
        "        super().__init__()\n",
        "        self.bert = bert\n",
        "        self.next_sentence = NextSentencePrediction(self.bert.d_model)\n",
        "        self.mask_lm = MaskedLanguageModel(self.bert.d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x, segment_label):\n",
        "        x = self.bert(x, segment_label)\n",
        "        return self.next_sentence(x), self.mask_lm(x)\n",
        "\n",
        "# Testing the BERT model and related components\n",
        "bert_model = BERT(len(tokenizer.vocab))\n",
        "bert_result = bert_model(sample_data['bert_input'], sample_data['segment_label'])\n",
        "print(bert_result.size())\n",
        "\n",
        "bert_lm = BERTLM(bert_model, len(tokenizer.vocab))\n",
        "final_result = bert_lm(sample_data['bert_input'], sample_data['segment_label'])\n",
        "print(final_result[0].size(), final_result[1].size())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4am76N6Cimj",
        "outputId": "efadd73d-2bec-428b-ffd1-d85934a04943"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 64, 768])\n",
            "torch.Size([32, 2]) torch.Size([32, 64, 21159])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ScheduledOptim():\n",
        "    def __init__(self, optimizer, d_model, n_warmup_steps):\n",
        "        \"\"\"\n",
        "        A wrapper class for learning rate scheduling.\n",
        "\n",
        "        :param optimizer: The inner optimizer (e.g., Adam).\n",
        "        :param d_model: The model's hidden dimension size.\n",
        "        :param n_warmup_steps: The number of warm-up steps for learning rate scheduling.\n",
        "        \"\"\"\n",
        "        self._optimizer = optimizer\n",
        "        self.n_warmup_steps = n_warmup_steps\n",
        "        self.n_current_steps = 0\n",
        "        self.init_lr = np.power(d_model, -0.5)\n",
        "\n",
        "    def step_and_update_lr(self):\n",
        "        \"\"\"\n",
        "        Step with the inner optimizer and update the learning rate.\n",
        "        \"\"\"\n",
        "        self._update_learning_rate()\n",
        "        self._optimizer.step()\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"\"\"\n",
        "        Zero out the gradients by the inner optimizer.\n",
        "        \"\"\"\n",
        "        self._optimizer.zero_grad()\n",
        "\n",
        "    def _get_lr_scale(self):\n",
        "        \"\"\"\n",
        "        Calculate the learning rate scale.\n",
        "        \"\"\"\n",
        "        return np.min([\n",
        "            np.power(self.n_current_steps, -0.5),\n",
        "            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n",
        "\n",
        "    def _update_learning_rate(self):\n",
        "        \"\"\"\n",
        "        Update the learning rate based on the learning rate scheduling per step.\n",
        "        \"\"\"\n",
        "        self.n_current_steps += 1\n",
        "        lr = self.init_lr * self._get_lr_scale()\n",
        "\n",
        "        for param_group in self._optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n"
      ],
      "metadata": {
        "id": "0PPi4L1sCjBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the class to train the model\n",
        "class BERTTrainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        train_dataloader,\n",
        "        test_dataloader=None,\n",
        "        lr=1e-4,\n",
        "        weight_decay=0.01,\n",
        "        betas=(0.9, 0.999),\n",
        "        warmup_steps=10000,\n",
        "        log_freq=10,\n",
        "        device='cuda',\n",
        "    ):\n",
        "        \"\"\"\n",
        "        BERT Trainer class for training BERT models.\n",
        "\n",
        "        :param model: BERT model to be trained.\n",
        "        :param train_dataloader: DataLoader for the training data.\n",
        "        :param test_dataloader: DataLoader for the test data (optional).\n",
        "        :param lr: Learning rate for optimization.\n",
        "        :param weight_decay: Weight decay for regularization.\n",
        "        :param betas: Betas for the Adam optimizer.\n",
        "        :param warmup_steps: Number of warm-up steps for learning rate scheduling.\n",
        "        :param log_freq: Logging frequency.\n",
        "        :param device: Device for training ('cuda' or 'cpu').\n",
        "        \"\"\"\n",
        "\n",
        "        self.device = device\n",
        "        self.model = model\n",
        "        self.train_data = train_dataloader\n",
        "        self.test_data = test_dataloader\n",
        "\n",
        "        # Setting the Adam optimizer with hyper-parameters\n",
        "        self.optim = Adam(self.model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
        "        self.optim_schedule = ScheduledOptim(self.optim, self.model.bert.d_model, n_warmup_steps=warmup_steps)\n",
        "\n",
        "        # Using Negative Log Likelihood Loss function for predicting the masked_token\n",
        "        self.criterion = torch.nn.NLLLoss(ignore_index=0)\n",
        "        self.log_freq = log_freq\n",
        "        print(\"Total Parameters:\", sum([p.nelement() for p in self.model.parameters()]))\n",
        "\n",
        "    def train(self, epoch):\n",
        "        self.iteration(epoch, self.train_data)\n",
        "\n",
        "    def test(self, epoch):\n",
        "        self.iteration(epoch, self.test_data, train=False)\n",
        "\n",
        "    def iteration(self, epoch, data_loader, train=True):\n",
        "        avg_loss = 0.0\n",
        "        total_correct = 0\n",
        "        total_element = 0\n",
        "\n",
        "        mode = \"train\" if train else \"test\"\n",
        "\n",
        "        # Progress bar\n",
        "        data_iter = tqdm.tqdm(\n",
        "            enumerate(data_loader),\n",
        "            desc=\"EP_%s:%d\" % (mode, epoch),\n",
        "            total=len(data_loader),\n",
        "            bar_format=\"{l_bar}{r_bar}\"\n",
        "        )\n",
        "\n",
        "        for i, data in data_iter:\n",
        "            # Move batch data to the specified device (GPU or CPU)\n",
        "            data = {key: value.to(self.device) for key, value in data.items()}\n",
        "\n",
        "            # Forward pass: Next sentence prediction and masked language model prediction\n",
        "            next_sent_output, mask_lm_output = self.model.forward(data[\"bert_input\"], data[\"segment_label\"])\n",
        "\n",
        "            # Calculate NLL loss for is_next classification result\n",
        "            next_loss = self.criterion(next_sent_output, data[\"is_next\"])\n",
        "\n",
        "            # Calculate NLLLoss for predicting masked token word\n",
        "            mask_loss = self.criterion(mask_lm_output.transpose(1, 2), data[\"bert_label\"])\n",
        "\n",
        "            # Add next_loss and mask_loss as described in the pre-training procedure\n",
        "            loss = next_loss + mask_loss\n",
        "\n",
        "            # Backward and optimization (only in training mode)\n",
        "            if train:\n",
        "                self.optim_schedule.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optim_schedule.step_and_update_lr()\n",
        "\n",
        "            # Calculate next sentence prediction accuracy\n",
        "            correct = next_sent_output.argmax(dim=-1).eq(data[\"is_next\"]).sum().item()\n",
        "            avg_loss += loss.item()\n",
        "            total_correct += correct\n",
        "            total_element += data[\"is_next\"].nelement()\n",
        "\n",
        "            post_fix = {\n",
        "                \"epoch\": epoch,\n",
        "                \"iter\": i,\n",
        "                \"avg_loss\": avg_loss / (i + 1),\n",
        "                \"avg_acc\": total_correct / total_element * 100,\n",
        "                \"loss\": loss.item()\n",
        "            }\n",
        "\n",
        "            if i % self.log_freq == 0:\n",
        "                data_iter.write(str(post_fix))\n",
        "        print(\n",
        "            f\"EP{epoch}, {mode}: \\\n",
        "            avg_loss={avg_loss / len(data_iter)}, \\\n",
        "            total_acc={total_correct * 100.0 / total_element}\"\n",
        "        )\n",
        "\n",
        "# Test example\n",
        "train_data = BERTDataset(pairs, seq_len=MAX_LEN, tokenizer=tokenizer)\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, pin_memory=True)\n",
        "bert_model = BERT(len(tokenizer.vocab))\n",
        "bert_lm = BERTLM(bert_model, len(tokenizer.vocab))\n",
        "bert_trainer = BERTTrainer(bert_lm, train_loader, device='cpu')\n",
        "epochs = 2\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    bert_trainer.train(epoch)\n"
      ],
      "metadata": {
        "id": "IEggGpx2kUXx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}